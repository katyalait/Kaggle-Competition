{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Machine learning assignement2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ktlait/Kaggle-Competition/blob/master/Machine_learning_assignement2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g9eqFUKIVl25",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sF06ykZ7VrEq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn import neural_network as nnet\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.linear_model import Lasso\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "from scipy import stats\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "\n",
        "NEURAL_NET = 0\n",
        "LINEAR_REGRESSION = 1\n",
        "GRADIENT_BOOST = 2\n",
        "LASSO_REGRESSION = 3\n",
        "XGBOOST = 4\n",
        "\n",
        "# Constant int values representing columns which are associated with\n",
        "# their value in the COLUMN array\n",
        "INSTANCE=0\n",
        "YEAR=1\n",
        "HOUSING=2\n",
        "CRIME=3\n",
        "WORK_EXPERIENCE=4\n",
        "SATISFACTION=5\n",
        "GENDER=6\n",
        "AGE=7\n",
        "COUNTRY=8\n",
        "CITY_SIZE=9\n",
        "PROFESSION=10\n",
        "DEGREE=11\n",
        "GLASSES=12\n",
        "HAIR=13\n",
        "HEIGHT=14\n",
        "ADD_INCOME=15\n",
        "INCOME=16\n",
        "\n",
        "NUM_PLOT = [CRIME, WORK_EXPERIENCE, CITY_SIZE, ADD_INCOME]\n",
        "STR_PLOT = [HOUSING, SATISFACTION,\n",
        "            COUNTRY, PROFESSION, DEGREE]\n",
        "\n",
        "CLEAN_DATA_DIR = os.path.abspath('data/clean_data')\n",
        "DATA_DIR = os.path.abspath('data')\n",
        "\n",
        "TRAINING_DATA = os.path.join(DATA_DIR, \"training_data.csv\")\n",
        "TEST_DATA = os.path.join(DATA_DIR, \"test_data.csv\")\n",
        "\n",
        "NA_COLUMNS = [YEAR, SATISFACTION, GENDER, COUNTRY, PROFESSION, DEGREE, HOUSING, WORK_EXPERIENCE]\n",
        "TARGET_COLUMNS = [INCOME, ADD_INCOME]\n",
        "CATEGORICAL_COLS = [SATISFACTION, GENDER, COUNTRY, PROFESSION, DEGREE, HOUSING]\n",
        "OH_COLS = [GENDER, DEGREE, SATISFACTION, HOUSING]\n",
        "ENCODING_COLS = [COUNTRY, PROFESSION]\n",
        "COLS_TO_TRANSFORM = [INCOME]\n",
        "LOW_FREQUENCY_THRESHOLD = 0\n",
        "DROPPED_COLUMNS = [GLASSES, HAIR]\n",
        "\n",
        "\n",
        "COLUMNS = ['Instance', 'Year', 'Housing', 'Crime','Work Experience', 'Satisfaction',\n",
        "       'Gender', 'Age', 'Country', 'Size', 'Profession',\n",
        "       'Degree', 'Glasses', 'Hair', 'Height',\n",
        "       'Additional Income', 'Income']\n",
        "\n",
        "MISSING_VALUES = ['#N/A', 'nA', '#NUM!']\n",
        "\n",
        "INCOME_OUTLIER_THRESHOLD = np.log(4000000)\n",
        "NUM_FOLDS = 3\n",
        "\n",
        "\n",
        "def rename_columns(df):\n",
        "    newnames = {\n",
        "        'Instance': 'Instance',\n",
        "        'Year of Record': 'Year',\n",
        "        'Housing Situation': 'Housing',\n",
        "        'Crime Level in the City of Employement': 'Crime',\n",
        "        'Work Experience in Current Job [years]': 'Work Experience',\n",
        "        'Satisfation with employer': 'Satisfaction',\n",
        "        'Size of City': 'Size',\n",
        "        'University Degree': 'Degree',\n",
        "        'Wears Glasses': 'Glasses',\n",
        "        'Hair Color': 'Hair',\n",
        "        'Body Height [cm]': 'Height',\n",
        "        'Yearly Income in addition to Salary (e.g. Rental Income)': 'Additional Income',\n",
        "        'Total Yearly Income [EUR]': 'Income'\n",
        "    }\n",
        "    df.rename(columns=newnames, inplace=True)\n",
        "    return df\n",
        "\n",
        "def get_df_from_csv(filename, training):\n",
        "    \"\"\"\n",
        "    Extracts and cleans the pandas datafram from a csv file\n",
        "    :param filename: string representation of file name\n",
        "    :param training: boolean indicating if the data is training or not\n",
        "    :return: pandas cleaned dataframe\n",
        "    \"\"\"\n",
        "    df = pd.read_csv(TRAINING_DATA, na_values=MISSING_VALUES, low_memory=False)\n",
        "    df = clean_data(df, training)\n",
        "    return df\n",
        "\n",
        "\n",
        "def clean_values(df):\n",
        "    print(\"Cleaning values\")\n",
        "    df[COLUMNS[GENDER]] = df[COLUMNS[GENDER]].replace(to_replace =\"f\", value =\"female\")\n",
        "    df[COLUMNS[GENDER]] = df[COLUMNS[GENDER]].replace(to_replace =\"m\", value =\"male\")\n",
        "    df[COLUMNS[GENDER]] = df[COLUMNS[GENDER]].replace(to_replace='0', value='unknown_Gender')\n",
        "    df[COLUMNS[DEGREE]] = df[COLUMNS[DEGREE]].replace(to_replace='0', value='No')\n",
        "    df[COLUMNS[HOUSING]] = df[COLUMNS[HOUSING]].replace(to_replace='0', value='unknown_Housing')\n",
        "    for col in df.columns:\n",
        "      val = \"uknown_\" + col\n",
        "      df[col] = df[col].replace(to_replace='unknown', value=val)\n",
        "    return df\n",
        "\n",
        "\n",
        "def oh_encode(df):\n",
        "    \"\"\"\n",
        "    One hot encodes columns in the dataframe\n",
        "    :param dataframe: string representation of file name\n",
        "    :return: pandas cleaned dataframe\n",
        "    \"\"\"\n",
        "    for col in OH_COLS:\n",
        "        df = pd.concat((df.drop(columns=COLUMNS[col]), pd.get_dummies(df[COLUMNS[col]], drop_first=True)), axis=1)\n",
        "        print(\"One hot encoding \" + COLUMNS[col])\n",
        "        print(df.shape)\n",
        "    return df\n",
        "\n",
        "def clean_str_cols(df, col):\n",
        "    \"\"\"\n",
        "    Replaces the NA values in the categorical columns with 'unknown'\n",
        "    :param df:\n",
        "    :param col:\n",
        "    :return df:\n",
        "\n",
        "    \"\"\"\n",
        "    print(\"filling unknown \" + COLUMNS[col])\n",
        "    replacement = 'unknown_' + COLUMNS[col]\n",
        "    df[COLUMNS[col]].fillna(replacement, inplace=True)\n",
        "    if df[COLUMNS[col]].isnull().values.any():\n",
        "        print(COLUMNS[col] + \" still has nans!\")\n",
        "    return df\n",
        "\n",
        "def clean_num_cols(df, col):\n",
        "    \"\"\"\n",
        "    Replaces the NA values in the numerical columsn with the mean of the column\n",
        "    :param df:\n",
        "    :param col:\n",
        "    :return df:\n",
        "    \"\"\"\n",
        "    df[COLUMNS[col]].fillna(df[COLUMNS[col]].mean(), inplace=True)\n",
        "    return df\n",
        "\n",
        "\n",
        "\n",
        "def create_target_mappings(df, target_column, encoding_columns, mean_smoothing_weight=0.3):\n",
        "        \"\"\"\n",
        "        Creates target mappings for columns in the provided dataframe\n",
        "        :param df:\n",
        "        :param target_column:\n",
        "        :param encoding_columns:\n",
        "        :param mean_smoothing_weight:\n",
        "        :return: target_maps\n",
        "        \"\"\"\n",
        "        target_mappings = {}\n",
        "        mean = df[COLUMNS[target_column]].mean()\n",
        "        target_mappings[COLUMNS[target_column]] = mean\n",
        "        for enc_col in encoding_columns:\n",
        "            agg = df.groupby(COLUMNS[enc_col])[COLUMNS[target_column]].agg(['count', 'mean'])\n",
        "            counts = agg['count']\n",
        "            means = agg['mean']\n",
        "\n",
        "            target_mappings[COLUMNS[enc_col]] = ((counts * means + mean_smoothing_weight * means)/(counts + mean_smoothing_weight))\n",
        "        return target_mappings\n",
        "\n",
        "def target_map_columns(df, target_maps, encoding_cols):\n",
        "    \"\"\"\n",
        "    For every target mapping in the provided target maps, it will map the values\n",
        "    of the corresponding columns in the df to the smoothed mean value\n",
        "    :param df:\n",
        "    :param target_maps:\n",
        "    :return df:\n",
        "    \"\"\"\n",
        "    for col in encoding_cols:\n",
        "        df[COLUMNS[col]] = df[COLUMNS[col]].map(target_maps[COLUMNS[col]]).fillna(target_maps[COLUMNS[INCOME]])\n",
        "    return df\n",
        "\n",
        "def encode_labels(df, encoding_cols):\n",
        "    \"\"\"\n",
        "    Label encodes the categorical cols passed in\n",
        "    :param df:\n",
        "    :param encoding_cols:\n",
        "    :return df:\n",
        "    \"\"\"\n",
        "\n",
        "    for col in encoding_cols:\n",
        "        label_encoder = LabelEncoder()\n",
        "        print(\"Encoding \" + COLUMNS[col])\n",
        "        print(df[COLUMNS[col]].head())\n",
        "        if df[COLUMNS[col]].isnull().values.any():\n",
        "            print(\"Found nulls!\")\n",
        "        df[COLUMNS[col]] = label_encoder.fit_transform(df[COLUMNS[col]])\n",
        "    return df\n",
        "\n",
        "\n",
        "def remove_unknowns(df, col, training):\n",
        "    \"\"\"\n",
        "    Removes all the unknowns from training data\n",
        "    :param df: pandas dataframe\n",
        "    :param col: column in which to remove rows with unknowns\n",
        "    :param training: boolean indicating if the data is training or not\n",
        "    :return: pandas cleaned dataframe\n",
        "    \"\"\"\n",
        "    if training:\n",
        "        df[COLUMNS[col]].fillna('nan', inplace=True)\n",
        "        df = df[df[COLUMNS[col]]!='nan']\n",
        "    else:\n",
        "        if col in CATEGORICAL_COLS:\n",
        "            clean_str_cols(df, col)\n",
        "        else:\n",
        "            clean_num_cols(df, col)\n",
        "    return df\n",
        "\n",
        "def log_transform(df, cols):\n",
        "    \"\"\"\n",
        "    Log transforms a column\n",
        "    :param df:\n",
        "    :param col:\n",
        "    :return: dataframe\n",
        "    \"\"\"\n",
        "    for col in cols:\n",
        "        df[COLUMNS[col]] = df[COLUMNS[col]].apply(np.log)\n",
        "    return df\n",
        "\n",
        "def untransform_col(df, cols):\n",
        "    \"\"\"\n",
        "    Reverse the log transform\n",
        "    :param df:\n",
        "    :param col:\n",
        "    :return: dataframe\n",
        "    \"\"\"\n",
        "    for col in cols:\n",
        "        df[COLUMNS[col]] = df[COLUMNS[col]].apply(np.exp)\n",
        "    return df\n",
        "\n",
        "def remove_outliers(df, training, col):\n",
        "    \"\"\"\n",
        "    Get rid of outliers in the column data\n",
        "    :param df:\n",
        "    :param training:\n",
        "    :param col:\n",
        "    :return: df\n",
        "    \"\"\"\n",
        "    if training:\n",
        "        z = np.abs(stats.zscore(df[COLUMNS[col]]))\n",
        "        df_w_o_outliers = df[(z < 5)]\n",
        "        return df_w_o_outliers\n",
        "    else:\n",
        "        return df\n",
        "\n",
        "def gradient_boosted_target_estimator(df):\n",
        "    pass\n",
        "\n",
        "def convert_sparse_values(df, threshold, cols, replacement='other'):\n",
        "    \"\"\"\n",
        "    Take a list of categorical columns in which to replace sparse values to\n",
        "    be represented as 'other'\n",
        "    :param df: dataframe\n",
        "    :param threshold: threshold value\n",
        "    :param cols: list of cols\n",
        "    :param replacement: value to convert sparse values to\n",
        "    :return: df\n",
        "    \"\"\"\n",
        "    for col in cols:\n",
        "        counts = df[COLUMNS[col]].value_counts()\n",
        "        sparse_val_indeces = counts[counts <= threshold].index\n",
        "        df[COLUMNS[col]] = df[COLUMNS[col]].replace(sparse_val_indeces, replacement)\n",
        "    return df\n",
        "\n",
        "\n",
        "\n",
        "#-----------------------Algorithms----------------------------------------\n",
        "def linear_regression(x, y, x_test, y_test, final):\n",
        "    \"\"\"\n",
        "    Basic linear regression algorithm\n",
        "    :param x:\n",
        "    :param y:\n",
        "    :param x_test:\n",
        "    :param y_test:\n",
        "    :param final:\n",
        "    :return y_pred:\n",
        "    \"\"\"\n",
        "    model = LinearRegression()\n",
        "    print(\"Creating model and fitting ...\")\n",
        "    model.fit(x, y)\n",
        "    print(\"Created model, predicting ...\")\n",
        "\n",
        "    y_pred = model.predict(final)\n",
        "    return y_pred\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def gradient_boost(x, y, x_test, y_test, submission_data):\n",
        "    \"\"\"\n",
        "    Create model using sklearn Gradient Boost Regressor.\n",
        "    :param x: training feature set\n",
        "    :param y: training target set\n",
        "    :param x_test: testing set from training data\n",
        "    :param y_test: testing target set from training data\n",
        "    :param submission_data: final test data set\n",
        "    :param iter_decreasing_change: number of iterations where MAE decreasing\n",
        "    :return y_pred: predicted targets\n",
        "    \"\"\"\n",
        "    lr_list = [0.05, 0.075, 0.1, 0.25, 0.5, 0.75, 1]\n",
        "    best_model = \"\"\n",
        "    min_score = 999999999999.9\n",
        "    for learning_rate in lr_list:\n",
        "        print(\"Learning rate: \", learning_rate)\n",
        "        n_estimators = 800\n",
        "        max_depth = 3\n",
        "        gb_clf = GradientBoostingRegressor(n_estimators=n_estimators,\n",
        "                    learning_rate=learning_rate, min_samples_split=20,\n",
        "                    max_depth=max_depth, random_state=0)\n",
        "        gb_clf.fit(x, y.ravel())\n",
        "        mae = mean_absolute_error(gb_clf.predict(x_test), y_test)\n",
        "        print(\"Mean Absolute Error (validation): {0:.3f}\".format(mae))\n",
        "        if mae < min_score:\n",
        "            best_model = gb_clf\n",
        "            min_score = mae\n",
        "        else:\n",
        "            iter_decreasing_change -= 1\n",
        "            if iter_decreasing_change==0:\n",
        "                break\n",
        "    return best_model.predict(submission_data)\n",
        "\n",
        "def lasso_regression(x_train, y_train, x_test, y_test, submission_data):\n",
        "    \"\"\"\n",
        "    Runs a lasso regression model on the input data\n",
        "    :param x_train:\n",
        "    :param y_train:\n",
        "    :param x_test:\n",
        "    :param y_test:\n",
        "    :param submission_data:\n",
        "    :return y_pred:\n",
        "    \"\"\"\n",
        "\n",
        "    lasso = Lasso()\n",
        "    #parameters = {'alpha':[1e-15, 1e-10, a1e-8, 1e-4, 1e-3, 1e-2, 1, 5, 10, 20]}\n",
        "    #lasso_regressor = GridSearchCV(lasso, parameters, scoring='neg_mean_absolute_error', cv=5)\n",
        "    #lasso_regressor.fit(x_train, y_train)\n",
        "    lasso.fit(x_train, y_train)\n",
        "    print(lasso.score(x_test, y_test))\n",
        "\n",
        "    return lasso.predict(submission_data)\n",
        "\n",
        "#-----------------------Helpers-------------------------------------------\n",
        "def create_n_net():\n",
        "    return nnet.MLPRegressor(\n",
        "        hidden_layer_sizes= (100,100,100,100,100),\n",
        "        max_iter= 6000,\n",
        "        tol=0.0000005,\n",
        "        n_iter_no_change=15,\n",
        "        warm_start=False,\n",
        "        early_stopping=True,\n",
        "        learning_rate=\"adaptive\",\n",
        "        learning_rate_init=0.000005)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def scale_model(x_train, x_test, submission_data):\n",
        "    \"\"\"\n",
        "    Scales data with minmaxscaler. Used for NN\n",
        "    :param x_train:\n",
        "    :param x_test:\n",
        "    :param submission_data:\n",
        "    :return x_train, x_test, submission_data:\n",
        "\n",
        "    \"\"\"\n",
        "    scaler = MinMaxScaler()\n",
        "    x_train = scaler.fit_transform(x_train)\n",
        "    x_test = scaler.fit_transform(x_test)\n",
        "    submission_data = scaler.fit_transform(submission_data)\n",
        "    return x_train, x_test, submission_data\n",
        "\n",
        "\n",
        "# prepend_drive = 'content/drive/My Drive/Colab Notebooks/data/'\n",
        "  \n",
        "# df = pd.read_csv(prepend_drive + 'training_data.csv', na_values=MISSING_VALUES, low_memory=False)\n",
        "# tf = pd.read_csv(prepend_drive + 'test_data.csv', na_values=MISSING_VALUES, low_memory=False)\n",
        "# df, tf = clean_data(df, tf)\n",
        "# model = train.split_and_train(df, TARGET_COLUMNS[0], 0.2, LASSO_REGRESSION, tf)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nVeGS9FiuN4X",
        "colab_type": "text"
      },
      "source": [
        "Create DFs from the files\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hxvTI2BeYEtr",
        "colab_type": "code",
        "outputId": "d57ad5d2-944c-4170-cf78-5d8f898ca141",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 995
        }
      },
      "source": [
        "import io\n",
        "from google.colab import drive, files\n",
        "drive.mount('/content/gdrive')\n",
        "#uploaded = files.upload()\n",
        "!ls \"/content/gdrive/My Drive/\"\n",
        "training_file = pd.read_csv(\"/content/gdrive/My Drive/training_data.csv\", na_values=MISSING_VALUES, low_memory=False)\n",
        "\n",
        "\n",
        "training_file.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "'Alex Dimiziani.gdoc'\n",
            " Doc.gdoc\n",
            "'Economics presentation.gslides'\n",
            "'encrypted_Bank statement.pdf'\n",
            "'encrypted_Colman Kinane'\n",
            "'encrypted_Colman Kinane (1).png'\n",
            "'encrypted_Colman Kinane.jpg'\n",
            " encrypted_Colman_Kinane.jpg.jpg\n",
            "'encrypted_Colman Kinane.png'\n",
            " encrypted_contract2.pdf\n",
            " encrypted_contract.pdf\n",
            " encrypted_error_test.png\n",
            " encrypted_example_file_name.jpg.jpg\n",
            " encrypted_example.png\n",
            " encrypted_Kate.jpg\n",
            " encrypted_Kate.png\n",
            " encrypted_sinead.png\n",
            "'encrypted_small_city (1).png'\n",
            " encrypted_small_city2\n",
            " encrypted_small_city.pdf\n",
            " encrypted_small_city.png\n",
            " encrypted_test1.png\n",
            " encrypted_test2.png\n",
            " encrypted_test3.png\n",
            " encrypted_vetting.pdf\n",
            "'Getting started.pdf'\n",
            " HubSpot.gdoc\n",
            "'Imagery in King Lear.gdoc'\n",
            " IMG_1185.MOV\n",
            " Interview.gdoc\n",
            "'Interview Transcript.gdoc'\n",
            "'Izzy Wheels.gdoc'\n",
            "'my Experiments.gsheet'\n",
            "'New List of Parts.gdoc'\n",
            " Phoebe.gdoc\n",
            "'Scientific Approach to Running Time.gsheet'\n",
            "'Shakespeareâ€™s King Lear.gdoc'\n",
            " small_city\n",
            "'TCP IP Model Notes.gdoc'\n",
            "'TES Email 2.gdoc'\n",
            "'tes email.gdoc'\n",
            "'TES EMAIL.gdoc'\n",
            " test_data.csv\n",
            " Ticketchain.gdoc\n",
            " Tickets.gdoc\n",
            "'Traffic Capture Report.gdoc'\n",
            " training_data.csv\n",
            " Tseliot_katelait.docx\n",
            "'Untitled document (1).gdoc'\n",
            "'Untitled document (2).gdoc'\n",
            "'Untitled document.gdoc'\n",
            "'Vegan Meal Plan.gdoc'\n",
            "'William Shu.gdoc'\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1048574, 17)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7_TdL6yUI-bT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "test_file = pd.read_csv(\"/content/gdrive/My Drive/test_data.csv\", na_values=MISSING_VALUES, low_memory=False)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nRjLNdqDwD_Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def clean_data(df, test):\n",
        "    \"\"\"\n",
        "    Cleans the dataframe\n",
        "    :param filename: string representation of file name\n",
        "    :param training: boolean indicating if the data is training or not\n",
        "    :return: pandas cleaned dataframe\n",
        "    \"\"\"\n",
        "    print(df.shape)\n",
        "    df['train'] = 1\n",
        "    test['train'] = 0\n",
        "\n",
        "    df = rename_columns(df)\n",
        "    test = rename_columns(test)\n",
        "\n",
        "    for col in DROPPED_COLUMNS:\n",
        "        df = df.drop(COLUMNS[col], axis=1)\n",
        "        test = test.drop(COLUMNS[col], axis=1)\n",
        "\n",
        "    for col in NA_COLUMNS:\n",
        "        df = remove_unknowns(df, col, True)\n",
        "        test = remove_unknowns(test, col, False)\n",
        "\n",
        "    total = pd.concat([df, test])\n",
        "    total = clean_values(total)\n",
        "    convert_sparse_values(total, LOW_FREQUENCY_THRESHOLD, CATEGORICAL_COLS)\n",
        "    total = oh_encode(total)\n",
        "    # Convert the additional income column to be numeric\n",
        "    total[COLUMNS[ADD_INCOME]] = total[COLUMNS[ADD_INCOME]].str.split(\" \", n=1, expand=True)[0]\n",
        "    total[COLUMNS[ADD_INCOME]] = pd.to_numeric(total[COLUMNS[ADD_INCOME]])\n",
        "\n",
        "    df = total[total['train']==1]\n",
        "    test = total[total['train']==0]\n",
        "\n",
        "    df = df.drop(['train'], axis=1)\n",
        "    test = test.drop(['train'], axis=1)\n",
        "\n",
        "\n",
        "    print(\"Mapping targets\")\n",
        "    df = remove_outliers(df, True, INCOME)\n",
        "    target_maps = create_target_mappings(df, INCOME, ENCODING_COLS)\n",
        "\n",
        "    df = target_map_columns(df, target_maps, ENCODING_COLS)\n",
        "    test = target_map_columns(test, target_maps, ENCODING_COLS)\n",
        "\n",
        "    # Make sure they have the same number of columns\n",
        "    print(df.shape)\n",
        "    print(test.shape)\n",
        "    return df, test\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HVH2fdYehRj1",
        "colab_type": "code",
        "outputId": "0f19a4b8-e5ca-48c5-a5a1-ad511d99f4ce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 703
        }
      },
      "source": [
        "df, tf = clean_data(training_file, test_file)\n",
        "tf.describe()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1048574, 17)\n",
            "filling unknown Satisfaction\n",
            "filling unknown Gender\n",
            "filling unknown Country\n",
            "filling unknown Profession\n",
            "filling unknown Degree\n",
            "filling unknown Housing\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/pandas/core/ops/__init__.py:1115: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
            "  result = method(y)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Cleaning values\n",
            "One hot encoding Gender\n",
            "(1131917, 19)\n",
            "One hot encoding Degree\n",
            "(1131917, 22)\n",
            "One hot encoding Satisfaction\n",
            "(1131917, 25)\n",
            "One hot encoding Housing\n",
            "(1131917, 31)\n",
            "Mapping targets\n",
            "(758464, 30)\n",
            "(369438, 30)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Instance</th>\n",
              "      <th>Year</th>\n",
              "      <th>Crime</th>\n",
              "      <th>Work Experience</th>\n",
              "      <th>Age</th>\n",
              "      <th>Country</th>\n",
              "      <th>Size</th>\n",
              "      <th>Profession</th>\n",
              "      <th>Height</th>\n",
              "      <th>Additional Income</th>\n",
              "      <th>Income</th>\n",
              "      <th>male</th>\n",
              "      <th>other</th>\n",
              "      <th>uknown_Gender</th>\n",
              "      <th>unknown_Gender</th>\n",
              "      <th>Master</th>\n",
              "      <th>No</th>\n",
              "      <th>PhD</th>\n",
              "      <th>unknown_Degree</th>\n",
              "      <th>Happy</th>\n",
              "      <th>Somewhat Happy</th>\n",
              "      <th>Unhappy</th>\n",
              "      <th>unknown_Satisfaction</th>\n",
              "      <th>Large Apartment</th>\n",
              "      <th>Large House</th>\n",
              "      <th>Medium Apartment</th>\n",
              "      <th>Medium House</th>\n",
              "      <th>Small Apartment</th>\n",
              "      <th>Small House</th>\n",
              "      <th>unknown_Housing</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>369438.000000</td>\n",
              "      <td>369438.000000</td>\n",
              "      <td>369438.000000</td>\n",
              "      <td>369438.000000</td>\n",
              "      <td>369438.000000</td>\n",
              "      <td>369438.000000</td>\n",
              "      <td>3.694380e+05</td>\n",
              "      <td>369438.000000</td>\n",
              "      <td>369438.000000</td>\n",
              "      <td>369438.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>369438.000000</td>\n",
              "      <td>369438.000000</td>\n",
              "      <td>369438.000000</td>\n",
              "      <td>369438.000000</td>\n",
              "      <td>369438.000000</td>\n",
              "      <td>369438.000000</td>\n",
              "      <td>369438.000000</td>\n",
              "      <td>369438.000000</td>\n",
              "      <td>369438.000000</td>\n",
              "      <td>369438.000000</td>\n",
              "      <td>369438.000000</td>\n",
              "      <td>369438.000000</td>\n",
              "      <td>369438.000000</td>\n",
              "      <td>369438.000000</td>\n",
              "      <td>369438.000000</td>\n",
              "      <td>369438.000000</td>\n",
              "      <td>369438.000000</td>\n",
              "      <td>369438.000000</td>\n",
              "      <td>369438.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>184719.500000</td>\n",
              "      <td>1979.511986</td>\n",
              "      <td>76.227822</td>\n",
              "      <td>16.136541</td>\n",
              "      <td>37.317133</td>\n",
              "      <td>75716.214369</td>\n",
              "      <td>8.298713e+05</td>\n",
              "      <td>73369.972106</td>\n",
              "      <td>175.215446</td>\n",
              "      <td>6708.241283</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.381585</td>\n",
              "      <td>0.240062</td>\n",
              "      <td>0.060744</td>\n",
              "      <td>0.077539</td>\n",
              "      <td>0.239632</td>\n",
              "      <td>0.244436</td>\n",
              "      <td>0.059918</td>\n",
              "      <td>0.077788</td>\n",
              "      <td>0.335052</td>\n",
              "      <td>0.147524</td>\n",
              "      <td>0.017140</td>\n",
              "      <td>0.036358</td>\n",
              "      <td>0.125277</td>\n",
              "      <td>0.124267</td>\n",
              "      <td>0.099096</td>\n",
              "      <td>0.125009</td>\n",
              "      <td>0.007384</td>\n",
              "      <td>0.124716</td>\n",
              "      <td>0.268722</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>106647.708712</td>\n",
              "      <td>23.050585</td>\n",
              "      <td>46.800595</td>\n",
              "      <td>5.575693</td>\n",
              "      <td>15.980679</td>\n",
              "      <td>24749.491934</td>\n",
              "      <td>2.118961e+06</td>\n",
              "      <td>8623.417382</td>\n",
              "      <td>19.951533</td>\n",
              "      <td>24048.123229</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.485776</td>\n",
              "      <td>0.427121</td>\n",
              "      <td>0.238860</td>\n",
              "      <td>0.267446</td>\n",
              "      <td>0.426859</td>\n",
              "      <td>0.429753</td>\n",
              "      <td>0.237335</td>\n",
              "      <td>0.267839</td>\n",
              "      <td>0.472009</td>\n",
              "      <td>0.354628</td>\n",
              "      <td>0.129791</td>\n",
              "      <td>0.187179</td>\n",
              "      <td>0.331033</td>\n",
              "      <td>0.329887</td>\n",
              "      <td>0.298792</td>\n",
              "      <td>0.330729</td>\n",
              "      <td>0.085614</td>\n",
              "      <td>0.330398</td>\n",
              "      <td>0.443295</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>1940.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.400000</td>\n",
              "      <td>14.000000</td>\n",
              "      <td>26053.575000</td>\n",
              "      <td>2.200000e+01</td>\n",
              "      <td>21676.333438</td>\n",
              "      <td>87.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>92360.250000</td>\n",
              "      <td>1960.000000</td>\n",
              "      <td>36.000000</td>\n",
              "      <td>12.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>62048.430560</td>\n",
              "      <td>7.277400e+04</td>\n",
              "      <td>67748.567735</td>\n",
              "      <td>160.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>184719.500000</td>\n",
              "      <td>1979.511986</td>\n",
              "      <td>75.000000</td>\n",
              "      <td>15.000000</td>\n",
              "      <td>35.000000</td>\n",
              "      <td>63224.065645</td>\n",
              "      <td>5.029655e+05</td>\n",
              "      <td>73271.255425</td>\n",
              "      <td>174.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>277078.750000</td>\n",
              "      <td>1999.000000</td>\n",
              "      <td>114.000000</td>\n",
              "      <td>20.000000</td>\n",
              "      <td>48.000000</td>\n",
              "      <td>79599.824278</td>\n",
              "      <td>1.183434e+06</td>\n",
              "      <td>78352.660080</td>\n",
              "      <td>190.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>369438.000000</td>\n",
              "      <td>2019.000000</td>\n",
              "      <td>204.000000</td>\n",
              "      <td>47.000000</td>\n",
              "      <td>126.000000</td>\n",
              "      <td>298698.567143</td>\n",
              "      <td>4.997078e+07</td>\n",
              "      <td>182189.667600</td>\n",
              "      <td>272.000000</td>\n",
              "      <td>162007.990000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "            Instance           Year  ...    Small House  unknown_Housing\n",
              "count  369438.000000  369438.000000  ...  369438.000000    369438.000000\n",
              "mean   184719.500000    1979.511986  ...       0.124716         0.268722\n",
              "std    106647.708712      23.050585  ...       0.330398         0.443295\n",
              "min         1.000000    1940.000000  ...       0.000000         0.000000\n",
              "25%     92360.250000    1960.000000  ...       0.000000         0.000000\n",
              "50%    184719.500000    1979.511986  ...       0.000000         0.000000\n",
              "75%    277078.750000    1999.000000  ...       0.000000         1.000000\n",
              "max    369438.000000    2019.000000  ...       1.000000         1.000000\n",
              "\n",
              "[8 rows x 30 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5TyqznlbvNv8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def format_to_csv(col_two, col_two_name, y_pred, filename, y_name='Total Yearly Income [EUR]'):\n",
        "    \"\"\"\n",
        "    Generates file with predicted values\n",
        "    :param instance_col: array of instance values\n",
        "    :param y_pred: array of predicted values\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    filename += \".csv\"\n",
        "    predicted = pd.DataFrame({col_two_name: col_two, y_name: y_pred})\n",
        "    predicted.to_csv(filename, header=[col_two_name, y_name], index=False)\n",
        "    print('Created csv!')\n",
        "\n",
        "def neural_net(x, y, x_test, y_test, submission_data):\n",
        "    \"\"\"\n",
        "    Create model using sklearn Neural Net Regressor\n",
        "    :param x: training feature set from training data\n",
        "    :param y: training target set from training data\n",
        "    :param x_test: testing feature set from training data\n",
        "    :param y_test: testing target set from training data\n",
        "    :param submission_data: testing data set\n",
        "    :return y_pred: predicted target values\n",
        "    \"\"\"\n",
        "    model_net = create_n_net()\n",
        "    print(\"Fitting model ...\")\n",
        "    model_net.fit(x, y)\n",
        "    print(\"Model fitted ...\")\n",
        "    print(\"Model training score: {0:.3f}\".format(model_net.score(x, y)))\n",
        "    print(\"Model testing score: {0:.3f}\".format(model_net.score(x_test, y_test)))\n",
        "    test_pred = model_net.predict(x_test)\n",
        "    format_to_csv(y_test, 'Actual', test_pred, 'test_predictions', 'Test Predictions')\n",
        "    return model_net.predict(submission_data)\n",
        "\n",
        "def split_and_train(df, target_col, split, algorithm, tf, file_end):\n",
        "    \"\"\"\n",
        "    Splits the data into training and test data based on the split provided\n",
        "    and calls the training algorithm to create a model\n",
        "    :param df: cleaned dataframe\n",
        "    :param split: split represented as a decimal\n",
        "    :param algorithm: index representing which algorithm to use\n",
        "    :return model: to be used for further testing\n",
        "    \"\"\"\n",
        "    print(\"Splitting data ...\")\n",
        "    scaler = MinMaxScaler()\n",
        "    x = df.drop([COLUMNS[target_col], COLUMNS[INSTANCE]], axis=1)\n",
        "    scaler.fit(x)\n",
        "    x = scaler.transform(x)\n",
        "    y = df[COLUMNS[target_col]]\n",
        "    submission_data = tf.drop([COLUMNS[target_col], COLUMNS[INSTANCE]], axis=1)\n",
        "    submission_data = scaler.transform(submission_data)\n",
        "    print(\"Submission data shape: \" + str(submission_data.shape))\n",
        "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=split)\n",
        "    print(\"X train: \" + str(x_train.shape))\n",
        "    print(\"X test: \" + str(x_test.shape))\n",
        "    model = \"\"\n",
        "    print(\"Running training algorithm ...\")\n",
        "    if algorithm==NEURAL_NET:\n",
        "      x_train, x_test, submission_data = scale_model(x_train, x_test, submission_data)\n",
        "      model = neural_net(x_train, y_train, x_test, y_test, submission_data)\n",
        "    elif algorithm==LINEAR_REGRESSION:\n",
        "      model = linear_regression(x_train, y_train, x_test, y_test, submission_data)\n",
        "    elif algorithm==GRADIENT_BOOST:\n",
        "      model = gradient_boost(x_train, y_train, x_test, y_test, submission_data, 2)\n",
        "    elif algorithm==LASSO_REGRESSION:\n",
        "      model = lasso_regression(x_train, y_train, x_test, y_test, submission_data)\n",
        "    elif algorithm==XGBOOST:\n",
        "      model = XGBoostTrain(x, y, submission_data)\n",
        "    elif algorithm==RANDOM_FOREST:\n",
        "      model = random_forest(x_train, y_train, x, y, submission_data)\n",
        "\n",
        "    model = pd.DataFrame({'Income': model.flatten()})\n",
        "    print(\"Shape of final prediction: \" + str(model.shape))\n",
        "    # Untransform the data\n",
        "    filename = \"submission_\" + str(file_end)\n",
        "    format_to_csv(tf[COLUMNS[INSTANCE]], COLUMNS[INSTANCE], model['Income'], filename)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8GQiRiUlhrR7",
        "colab_type": "code",
        "outputId": "f5dc6653-9d03-4987-e9f6-271ce59fc2da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "RANDOM_FOREST = 5\n",
        "\n",
        "target_col = INCOME\n",
        "scaler = MinMaxScaler()\n",
        "x = df.drop([COLUMNS[target_col], COLUMNS[INSTANCE]], axis=1)\n",
        "scaler.fit(x)\n",
        "x = scaler.transform(x)\n",
        "y = df[COLUMNS[target_col]]\n",
        "submission_data = tf.drop([COLUMNS[target_col], COLUMNS[INSTANCE]], axis=1)\n",
        "submission_data = scaler.transform(submission_data)\n",
        "print(\"Submission data shape: \" + str(submission_data.shape))\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)\n",
        "print(\"X train: \" + str(x_train.shape))\n",
        "feature_list = list(df.columns)\n",
        "print(feature_list)\n",
        "rf = RandomForestRegressor(n_estimators = 170, criterion=\"mae\", random_state = 42)\n",
        "rf.fit(x_train, y_train)\n",
        "predictions = rf.predict(x_test)\n",
        "errors = abs(predictions - y_test)\n",
        "print('Mean Absolute Error:', round(np.mean(errors), 2), 'degrees.')\n",
        "  #important_indices = [feature_list.index('temp_1'), feature_list.index('average')]\n",
        "  #train_important = train_features[:, important_indices]\n",
        "  #test_important = test_features[:, important_indices]\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Submission data shape: (369438, 28)\n",
            "X train: (606771, 28)\n",
            "['Instance', 'Year', 'Crime', 'Work Experience', 'Age', 'Country', 'Size', 'Profession', 'Height', 'Additional Income', 'Income', 'male', 'other', 'uknown_Gender', 'unknown_Gender', 'Master', 'No', 'PhD', 'unknown_Degree', 'Happy', 'Somewhat Happy', 'Unhappy', 'unknown_Satisfaction', 'Large Apartment', 'Large House', 'Medium Apartment', 'Medium House', 'Small Apartment', 'Small House', 'unknown_Housing']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r11W2tb6EXpE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_pred = rf.predict(submission_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4FChF5MEEo5S",
        "colab_type": "code",
        "outputId": "c8ddb421-6612-4532-9a60-38654bf1cdc2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "model = pd.DataFrame({'Income': y_pred.flatten()})\n",
        "format_to_csv(tf[COLUMNS[INSTANCE]], COLUMNS[INSTANCE], model['Income'], \"prediction\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Created csv!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IjPGiPUGFh9P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "files.download('prediction.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7SZAMoekzWgq",
        "colab_type": "code",
        "outputId": "5e58921c-52aa-4f6c-96ef-b783ead3816f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "XGBOOST=4\n",
        "import xgboost as xgb\n",
        "\n",
        "def XGBoostTrain(x, y, tf):\n",
        "  xgtrain = xgb.DMatrix(data=x.values,label=y.values)\n",
        "  xgtest = xgb.DMatrix(tf.values)\n",
        "\n",
        "  X_train, X_test, y_train, y_test = train_test_split(x.values, y.values, test_size=0.2, random_state=123)\n",
        "  xg_reg = xgb.XGBRegressor(objective ='reg:squarederror', learning_rate = 0.5, \n",
        "                            max_depth = 6, n_estimators = 500, \n",
        "                            subsample=0.8)\n",
        "  xg_reg.fit(X_train, y_train)\n",
        "  preds = xg_reg.predict(X_test)\n",
        "  print(\"Model training score: {0:.3f}\".format(mean_absolute_error(y_test, preds)))\n",
        "  print(tf.shape)\n",
        "  y_pred = xg_reg.predict(tf.values)\n",
        "  print(y_pred.shape)\n",
        "\n",
        "  # params = {\"objective\":\"reg:squarederror\",'colsample_bytree': 0.3,'learning_rate': 0.1,\n",
        "  #               'max_depth': 5, 'alpha': 10}\n",
        "\n",
        "  # cv_results = xgb.cv(dtrain=xgtrain, params=params, nfold=3, \n",
        "  #                     num_boost_round=50,early_stopping_rounds=10,metrics=\"rmse\", as_pandas=True, seed=123)\n",
        "  # print((cv_results[\"test-rmse-mean\"]).tail(1))\n",
        "\n",
        "  xgb.plot_importance(xg_reg)\n",
        "  plt.rcParams['figure.figsize'] = [5, 5]\n",
        "  plt.show()\n",
        "  return y_pred\n",
        "\n",
        "split_and_train(df, TARGET_COLUMNS[0], 0.2, RANDOM_FOREST, tf, 4)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Splitting data ...\n",
            "Submission data shape: (369438, 30)\n",
            "X train: (606771, 30)\n",
            "X test: (151693, 30)\n",
            "Running training algorithm ...\n",
            "['Instance', 'Year', 'Crime', 'Work Experience', 'Age', 'Country', 'Size', 'Profession', 'Height', 'Additional Income', 'Income', 'female', 'male', 'other', 'unknown', 'unknown_gender', 'Master', 'No', 'PhD', 'Unknown_Degree', 'Happy', 'Somewhat Happy', 'Unhappy', 'Unknown_Satisfaction', 'Large Apartment', 'Large House', 'Medium Apartment', 'Medium House', 'Small Apartment', 'Small House', 'Unknown_Housing', 'unknown_housing']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qgR4VHs6iCSq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "files.download('submission_4.csv')\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}